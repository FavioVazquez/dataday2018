{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering with Optimus\n",
    "==================================\n",
    "\n",
    "When we talk about Feature Engineering we refer to creating new features from your existing ones to improve model\n",
    "performance. Sometimes this is the case, or sometimes you need to do it because a certain model doesn't recognize\n",
    "the data as you have it, so these transformations let you run most of Machine and Deep Learning algorithms.\n",
    "\n",
    "These methods are part of the DataFrameTransformer, and they are a high level of abstraction for Spark Feature\n",
    "Engineering methods. You'll see how easy it is to prepare your data with Optimus for Machine Learning.\n",
    "\n",
    "\n",
    "Methods for Feature Engineering\n",
    "---------------------------------\n",
    "\n",
    "## - Transformer.string_to_index(input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it\n",
    "to string and index the string values.\n",
    "\n",
    "``input_cols`` argument receives a list of columns to be indexed.\n",
    "\n",
    "Let's start by creating a DataFrame with Optimus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>Starting or getting SparkSession and SparkContext.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Setting checkpoint folder (local). If you are in a cluster change it with set_check_point_folder(path,'hadoop').</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting previous folder if exists...\n",
      "Creation of checkpoint directory...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ironmussa/Optimus\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ironmussa/Optimus/raw/master/images/robotOptimus.png\" style=\"float:left;margin-right:10px;vertical-align:top;text-align:center\" height=\"50\" width=\"50\"/>\n",
       "            </a>\n",
       "            <span><b><h2>Optimus successfully imported. Have fun :).</h2></b></span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Optimus\n",
    "import optimus as op\n",
    "#Importing utilities\n",
    "tools = op.Utilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|country|    city|population|\n",
      "+-------+--------+----------+\n",
      "|  Japan|   Tokyo|  37800000|\n",
      "|    USA|New York|  19795791|\n",
      "| France|   Paris|  12341418|\n",
      "|  Spain|  Madrid|   6489162|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DF with Optimus\n",
    "data = [('Japan', 'Tokyo', 37800000),('USA', 'New York', 19795791),('France', 'Paris', 12341418),\n",
    "              ('Spain','Madrid',6489162)]\n",
    "df = tools.create_data_frame(data, [\"country\", \"city\", \"population\"])\n",
    "\n",
    "# Instantiating transformer\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "# Show DF\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+-------------+\n",
      "|country|    city|population|city_index|country_index|\n",
      "+-------+--------+----------+----------+-------------+\n",
      "|  Japan|   Tokyo|  37800000|       1.0|          1.0|\n",
      "|    USA|New York|  19795791|       2.0|          3.0|\n",
      "| France|   Paris|  12341418|       3.0|          2.0|\n",
      "|  Spain|  Madrid|   6489162|       0.0|          0.0|\n",
      "+-------+--------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing columns 'city\" and 'country'\n",
    "transformer.string_to_index([\"city\", \"country\"])\n",
    "\n",
    "# Show indexed DF\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformer.index_to_string(input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method maps a column of indices back to a new column of corresponding string values. The index-string mapping is\n",
    "either from the ML (Spark) attributes of the input column, or from user-supplied labels (which take precedence over\n",
    "ML attributes).\n",
    "\n",
    "``input_cols`` argument receives a list of columns to be indexed.\n",
    "\n",
    "Let's go back to strings with the DataFrame we created in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|country|    city|population|\n",
      "+-------+--------+----------+\n",
      "|  Japan|   Tokyo|  37800000|\n",
      "|    USA|New York|  19795791|\n",
      "| France|   Paris|  12341418|\n",
      "|  Spain|  Madrid|   6489162|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiating transformer\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "# Show DF\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+-------------+\n",
      "|country|    city|population|city_index|country_index|\n",
      "+-------+--------+----------+----------+-------------+\n",
      "|  Japan|   Tokyo|  37800000|       1.0|          1.0|\n",
      "|    USA|New York|  19795791|       2.0|          3.0|\n",
      "| France|   Paris|  12341418|       3.0|          2.0|\n",
      "|  Spain|  Madrid|   6489162|       0.0|          0.0|\n",
      "+-------+--------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing columns 'city\" and 'country'\n",
    "transformer.string_to_index([\"city\", \"country\"])\n",
    "\n",
    "# Show indexed DF\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+-------------+--------------------+\n",
      "|country|    city|population|city_index|country_index|country_index_string|\n",
      "+-------+--------+----------+----------+-------------+--------------------+\n",
      "|  Japan|   Tokyo|  37800000|       1.0|          1.0|               Japan|\n",
      "|    USA|New York|  19795791|       2.0|          3.0|                 USA|\n",
      "| France|   Paris|  12341418|       3.0|          2.0|              France|\n",
      "|  Spain|  Madrid|   6489162|       0.0|          0.0|               Spain|\n",
      "+-------+--------+----------+----------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Going back to strings from index\n",
    "transformer.index_to_string([\"country_index\"])\n",
    "\n",
    "# Show DF with column \"county_index\" back to string\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformer.one_hot_encoder(input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method maps a column of label indices to a column of binary vectors, with at most a single one-value.\n",
    "\n",
    "``input_cols`` argument receives a list of columns to be encoded.\n",
    "\n",
    "Let's create a sample dataframe to see what OHE does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "~"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|   id_encoded|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|(5,[0],[1.0])|\n",
      "|  1|       b|(5,[1],[1.0])|\n",
      "|  2|       c|(5,[2],[1.0])|\n",
      "|  3|       a|(5,[3],[1.0])|\n",
      "|  4|       a|(5,[4],[1.0])|\n",
      "|  5|       c|    (5,[],[])|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "data = [\n",
    "(0, \"a\"),\n",
    "(1, \"b\"),\n",
    "(2, \"c\"),\n",
    "(3, \"a\"),\n",
    "(4, \"a\"),\n",
    "(5, \"c\")\n",
    "]\n",
    "df = tools.create_data_frame(data,[\"id\", \"category\"])\n",
    "\n",
    "# Instantiating the transformer\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "# One Hot Encoding\n",
    "transformer.one_hot_encoder([\"id\"])\n",
    "\n",
    "# Show encoded dataframe\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformer.sql(sql_expression)\n",
    "\n",
    "This method implements the transformations which are defined by SQL statement. Spark only support\n",
    "SQL syntax like \"SELECT ... FROM __THIS__ ...\" where \"__THIS__\" represents the\n",
    "underlying table of the input dataframe. Thank Spark for this amazing function.\n",
    "\n",
    "`sql_expression` argument receives a string that contains SQL expression.\n",
    "\n",
    "Let's create a sample DataFrame to test this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id| v1| v2|\n",
      "+---+---+---+\n",
      "|  0|1.0|3.0|\n",
      "|  2|2.0|5.0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "data = [\n",
    "(0, 1.0, 3.0),\n",
    "(2, 2.0, 5.0)\n",
    "]\n",
    "\n",
    "df = tools.create_data_frame(data,[\"id\", \"v1\", \"v2\"])\n",
    "\n",
    "# Instantiating the transformer\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create two new columns from these ones. The first will be the sum of the columns `v1` and `v2`, and\n",
    "the second one will be the multiplication of this two columns. With the `sql()` function we just need to\n",
    "pass the sql expression and use at the end `FROM __THIS__` that will be the underlying table of the input dataframe.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optimus.df_transformer.DataFrameTransformer at 0x104e8b6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.sql(\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformer.vector_assembler(input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method combines a given list of columns into a single vector column.\n",
    "\n",
    "``input_cols`` argument receives a list of columns to be encoded.\n",
    "\n",
    "This is very important because lots of Machine Learning algorithms in Spark need this format to work.\n",
    "\n",
    "Let's create a sample dataframe to see what vector assembler does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+-------+\n",
      "| id|hour|mobile| user_features|clicked|\n",
      "+---+----+------+--------------+-------+\n",
      "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
      "+---+----+------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Creating DataFrame\n",
    "data = [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)]\n",
    "\n",
    "df = tools.create_data_frame(data,[\"id\", \"hour\", \"mobile\", \"user_features\", \"clicked\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'user_features' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the transformer\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "# Assemble features\n",
    "transformer.vector_assembler([\"hour\", \"mobile\", \"user_features\"])\n",
    "\n",
    "\n",
    "# Show assembled df\n",
    "print(\"Assembled columns 'hour', 'mobile', 'user_features' to vector column 'features'\")\n",
    "transformer.df.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformer.normalizer(input_cols,p=2.0)\n",
    "\n",
    "\n",
    "This method transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which\n",
    "specifies the p-norm used for normalization. (p=2) by default.\n",
    "\n",
    "``input_cols`` argument receives a list of columns to be normalized.\n",
    "\n",
    "``p`` argument is the p-norm used for normalization.\n",
    "\n",
    "Let's create a sample dataframe to see what normalizer does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]|\n",
      "|  2|[4.0,10.0,2.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing Optimus\n",
    "import optimus as op\n",
    "#Importing utilities\n",
    "tools = op.Utilities()\n",
    "# Import Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "(0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "(2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "]\n",
    "\n",
    "df = tools.create_data_frame(data,[\"id\", \"features\"])\n",
    "\n",
    "transformer = op.DataFrameTransformer(df)\n",
    "\n",
    "transformer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------+\n",
      "|id |features      |features_normalized|\n",
      "+---+--------------+-------------------+\n",
      "|0  |[1.0,0.5,-1.0]|[0.4,0.2,-0.4]     |\n",
      "|1  |[2.0,1.0,1.0] |[0.5,0.25,0.25]    |\n",
      "|2  |[4.0,10.0,2.0]|[0.25,0.625,0.125] |\n",
      "+---+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.normalizer([\"features\"], p=1.0).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
